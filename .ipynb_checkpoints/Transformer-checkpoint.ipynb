{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9628c6e7",
   "metadata": {},
   "source": [
    "In this article Transformer Network is implemented in Pytorch based on \"Attention is all you need\" paper. \n",
    "The first motivation of developing such network that is the sequential analysis is a bottleneck in sequential datas. What a transformer network is trying to do, is combining CNN and RNN specially attention mechanism, create a network that solves this bottelneck.\n",
    "The network consist of some parts.\n",
    "1. self attention:\n",
    "    an attention mechanism relating to diffrent parts of a sequence.\n",
    "2. Encoder\n",
    "    the encoder block inputs (x1,...,xn) and outputs (z1,...,zn) and the decoder generates (y1,...,yn) based on input Z. Also encoder block consists of N=6 identical layers.\n",
    "    the encoder consists of two parts. a multi-head attention and a feed forward network. and some residual connections between them. The residual is a normalization layer over output of each block plus output of the privious block.\n",
    "    note that input dimentions (embedding dimention) of encoder is d=512 and to make use of residual connections we have to set output of each block to have same dimention of input so dimention of output of each block is d=512.\n",
    "3. Decoder:\n",
    "    The decoder is like encoder consists except it has a new multi-head layer to relate input of decoder to encoder features.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1f7dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pdb\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import repeat\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a51f84",
   "metadata": {},
   "source": [
    "First of all we need a scaled dot product. Typically what this layer does is that it computes softmax(Q * K.T / sqrt(d_k)) * V. This computes the value of output in regard to each query and key (question and answers which are created during training). multiplying with value.\n",
    "The shapes are like this. Q, K:(batch_size, sequence_number, dk) and V:(batch_size, sequence_number, dv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b02445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        self.activation = nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        x = torch.bmm(Q, K.transpose(-1, -2))\n",
    "        dk = torch.tensor(K.size(-1))\n",
    "        x = x.div(torch.sqrt(dk))\n",
    "        x = self.activation(x)\n",
    "        x = torch.bmm(x, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f35ae1",
   "metadata": {},
   "source": [
    "in order to test This class I create this paramters. Work on paper to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d78018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "def test_scaled_dot_product():\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    d_k = 4\n",
    "    d_v = 6\n",
    "    Q = torch.full((batch_size, sequence_number, d_k), 1, dtype= torch.float)\n",
    "    K = torch.full((batch_size, sequence_number, d_k), 2, dtype= torch.float)\n",
    "    V = torch.full((batch_size, sequence_number, d_v), 3, dtype= torch.float)\n",
    "    scaled_dot_product = ScaledDotProduct()\n",
    "    product = scaled_dot_product(Q, K, V)\n",
    "#     torch_versio = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    print(product.size())\n",
    "#     print(torch_versio)\n",
    "test_scaled_dot_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6ef35",
   "metadata": {},
   "source": [
    "ok, we implemented scaled dot product. Now we need to implement multi-head attention using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5df403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dk = 10\n",
    "        self.dv = 12\n",
    "        self.num_head = num_head\n",
    "        self.WQ = nn.Parameter(torch.randn(self.num_head, d_model, self.dk))\n",
    "        self.WK = nn.Parameter(torch.randn(self.num_head, d_model, self.dk))\n",
    "        self.WV = nn.Parameter(torch.randn(self.num_head, d_model, self.dv))\n",
    "        self.WO = nn.Parameter(torch.randn(self.num_head * self.dv, d_model))\n",
    "        self.reset_parameters()\n",
    "        self.scaled_dot_product = ScaledDotProduct()\n",
    "        \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.WQ)\n",
    "        nn.init.xavier_uniform_(self.WK)\n",
    "        nn.init.xavier_uniform_(self.WV)\n",
    "        \n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            WQi, WKi, WVi = self.WQ[i, :, :], self.WK[i, :, :], self.WV[i, :, :]\n",
    "            q = torch.bmm(Q, WQi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            k = torch.bmm(K, WKi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            v = torch.bmm(V, WVi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            heads.append(self.scaled_dot_product(q,k,v))\n",
    "        out = torch.cat(heads, dim=-1)\n",
    "        out = torch.bmm(out, self.WO.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb3c9a4",
   "metadata": {},
   "source": [
    "In order to test the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3dd37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    num_head = 8\n",
    "    d_model = 128\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    multi_head_attention = MultiHeadAttention(num_head, d_model)\n",
    "    Q = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    K = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    V = torch.full((batch_size, sequence_number, d_model), 3, dtype= torch.float)\n",
    "    out = multi_head_attention(Q, K, V)\n",
    "    print(out.size())\n",
    "    # use torch version to see if we were correct\n",
    "#     torch_multi_head_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_head, batch_first=True)\n",
    "#     torch_out = torch_multi_head_attention(Q, K, V, need_weights=False)\n",
    "#     print(torch_out[0])\n",
    "#     print(torch.eq(out, torch_out[0]).all())\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0ca78",
   "metadata": {},
   "source": [
    "The next block is a simple feed forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f62f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.layer2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.size()\n",
    "        x = x.view(-1, self.d_model)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a040426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_feed_forward():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, num_head, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    feed_forward = FeedForward(d_model, d_ff)\n",
    "    out = feed_forward(x)\n",
    "    print(out.size())\n",
    "test_feed_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7df76",
   "metadata": {},
   "source": [
    "Now we have all bulding blocks to create encoder and decoder. The residual connections are applied too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958e8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_head, d_model, d_ff):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.num_head, self.d_model, self.d_ff = num_head, d_model, d_ff\n",
    "        self.multi_head_attention1 = MultiHeadAttention(num_head, d_model)\n",
    "        self.feed_forward1 = FeedForward(d_model, d_ff)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "    def batch_norm(self, x):\n",
    "        x_temp = x.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1d(x_temp)\n",
    "        x = x_temp.view(x.size())\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.multi_head_attention1(x, x, x)\n",
    "        x1 = torch.add(x, x1)\n",
    "        \n",
    "        x_temp = x1.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1(x_temp)\n",
    "        x1 = x_temp.view(x1.size())\n",
    "        \n",
    "        x2 = self.feed_forward1(x1)\n",
    "        x2 = torch.add(x2, x1)\n",
    "        \n",
    "        x_temp = x2.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm2(x_temp)\n",
    "        x2 = x_temp.view(x2.size())\n",
    "        \n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcba99",
   "metadata": {},
   "source": [
    "and in order to test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3314068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_block():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder_block = EncoderBlock(num_head, d_model, d_ff)\n",
    "    out = encoder_block(x)\n",
    "    print(out.size())\n",
    "test_encoder_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae251dc7",
   "metadata": {},
   "source": [
    "In the next block, we implement the stack of encoder blocks. N number of blocks are stacked on the top of each other to create Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c36b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.block_list = nn.ModuleList([EncoderBlock(num_head, d_model, d_ff) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.block_list:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a006a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder = Encoder(N, num_head, d_model, d_ff)\n",
    "    out = encoder(x)\n",
    "    print(out.size())\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eec78",
   "metadata": {},
   "source": [
    "The exact same process is applied on decoder. Note that decoder inputs are the input sentence and the output of encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05801ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_head, d_model, d_ff):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_head, self.d_model, self.d_ff = num_head, d_model, d_ff\n",
    "        self.multi_head_attention1 = MultiHeadAttention(num_head, d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(num_head, d_model)\n",
    "        self.feed_forward1 = FeedForward(d_model, d_ff)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, encoder_out):\n",
    "        x1 = self.multi_head_attention1(x, x, x)\n",
    "        x1 = torch.add(x, x1)\n",
    "        \n",
    "        x_temp = x1.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1(x_temp)\n",
    "        x1 = x_temp.view(x1.size())\n",
    "        \n",
    "        x2 = self.multi_head_attention2(x1, encoder_out, encoder_out)\n",
    "        x2 = torch.add(x1, x2)\n",
    "        \n",
    "        x_temp = x2.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm2(x_temp)\n",
    "        x2 = x_temp.view(x2.size())\n",
    "        \n",
    "        x3 = self.feed_forward1(x2)\n",
    "        x3 = torch.add(x2, x3)\n",
    "        \n",
    "        x_temp = x3.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm3(x_temp)\n",
    "        x3 = x_temp.view(x3.size())\n",
    "        \n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00e57b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_block():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encode_out = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    decoder_block = DecoderBlock(num_head, d_model, d_ff)\n",
    "    out = decoder_block(x, encode_out)\n",
    "    print(out.size())\n",
    "test_encoder_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b84c6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.block_list = nn.ModuleList([DecoderBlock(num_head, d_model, d_ff) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x, encoder_out):\n",
    "        for block in self.block_list:\n",
    "            x = block(x, encoder_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3560e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_decoder():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder_out = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    decoder = Decoder(N, num_head, d_model, d_ff)\n",
    "    out = decoder(x, encoder_out)\n",
    "    print(out.size())\n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a588ea",
   "metadata": {},
   "source": [
    "Now its time to put everything together and create the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98d9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(N, num_head, d_model, d_ff)\n",
    "        self.decoder = Decoder(N, num_head, d_model, d_ff)\n",
    "        \n",
    "    def forward(self, x_encoder, x_decoder):\n",
    "        encoder_out = self.encoder(x_encoder)\n",
    "        out = self.decoder(x_decoder, encoder_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66c9dfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_transformer():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    \n",
    "    x_encoder = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    x_decoder = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    transformer = Transformer(N, num_head, d_model, d_ff)\n",
    "    out = transformer(x_encoder, x_decoder)\n",
    "    \n",
    "    print(out.size())\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ced7e",
   "metadata": {},
   "source": [
    "Now the model is created. We have to see how to feed data. The first step is to create word embedding. function below created word embedding. Note that \\<pad> and and \\<unk> tokens are not in glove and we add them as pad as zeros and unk as mean of all vectors. The \\<unk> part is now commented because sounds like glove solved it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b31efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(path: str, pad=False, unk=False):\n",
    "    vocab, embeddings = dict(), list()\n",
    "    with open(f'{path}', 'rt') as f:\n",
    "        full_content = f.read().strip().split('\\n')\n",
    "    for i in range(len(full_content)):\n",
    "        i_word = full_content[i].split(' ')[0]\n",
    "        i_embedding = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "        vocab[i_word] = i\n",
    "        embeddings.append(i_embedding)\n",
    "    embs_npa = np.array(embeddings)\n",
    "    if pad:\n",
    "        vocab['<pad>'] = len(vocab)\n",
    "        pad_emb_npa = np.zeros((1, embs_npa.shape[1]))\n",
    "        embs_npa = np.vstack((embs_npa, pad_emb_npa))\n",
    "    if unk:\n",
    "        vocab['<unk>'] = len(vocab)\n",
    "        unk_emb_npa = np.mean(embs_npa, axis=0, keepdims=True)\n",
    "        embs_npa = np.vstack((embs_npa, unk_emb_npa))\n",
    "    return vocab, embs_npa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e2ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab, eng_emb = get_embedding(\"./word2vec_eng/glove.6B.100d.txt\", pad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f19e4",
   "metadata": {},
   "source": [
    "In this model we create a layer that accepts a sentence and give its tokenized outputs. for the sake of learning this layer is not efficient. But it should be enhanced by torchtext for tokenizing and dataloader for input a sentence and output the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0914f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_vectors):\n",
    "        super(NaiveEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embedding_vectors).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, sentence):\n",
    "        return self.embedding(torch.from_numpy(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "354df4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n"
     ]
    }
   ],
   "source": [
    "def naive_embedding_test():\n",
    "    sentence = \"naive test\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.split()        \n",
    "    sentence = np.array([eng_vocab[x] for x in sentence])\n",
    "    embedding = NaiveEmbedding(eng_emb)\n",
    "    out = embedding(sentence)\n",
    "    print(out.size())\n",
    "naive_embedding_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa81469",
   "metadata": {},
   "source": [
    "But this is not the best way to do it. We create a dataset using our sentence datas and turn these samples into numeric data inside dataset. Lets load English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea884fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path):\n",
    "    with open(f'{train_path}/train.en') as f:\n",
    "        train_data_x = f.read().strip().split('\\n')\n",
    "    return train_data_x\n",
    "train_data_x = load_data(\"./train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b032d9",
   "metadata": {},
   "source": [
    "We create a torch dataset to obtain data from text dataset. all padding and adding adding unkown using can be done in this class. Output vectors are padded to maximum size of sentence. it can be changed in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7875994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, vocab2index, data):\n",
    "        self.data = data\n",
    "        self.vocab2index = vocab2index\n",
    "        self.max_len = self._get_max_len()\n",
    "        \n",
    "    def _get_max_len(self):\n",
    "        sentence_list = [x.split() for x in self.data]\n",
    "        len_list = (len(x) for x in sentence_list)\n",
    "        return max(len_list)\n",
    "        \n",
    "    def _text_to_index(self, text):\n",
    "        indexes = np.zeros((self.max_len,))\n",
    "        text = text.lower().split()\n",
    "        indexes[:len(text)] = np.array([self.vocab2index[x] if x in self.vocab2index else self.vocab2index[\"<unk>\"] for x in text])\n",
    "        pad = list()\n",
    "        pad.extend(repeat(self.vocab2index[\"<pad>\"],self.max_len - len(text)))\n",
    "        indexes[len(text):] = pad\n",
    "        indexes = indexes.astype(np.int32)\n",
    "        return indexes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        text = self.data[ind]\n",
    "        indexes = self._text_to_index(text)\n",
    "        return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0624441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_length, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.d_model = d_model\n",
    "        self.positional_encodeing = self._positional_encoding_maker()\n",
    "        \n",
    "    def _positional_encoding_maker(self):\n",
    "        positional_encodeing = torch.zeros(self.max_seq_length,self.d_model)\n",
    "        pos = torch.arange(0, self.max_seq_length).unsqueeze(1)\n",
    "        divide_val = torch.exp(torch.arange(0, self.d_model, 2) * -(math.log(1000.0) / self.d_model))\n",
    "        positional_encodeing[:,0::2] = torch.sin(pos * divide_val)\n",
    "        positional_encodeing[:,1::2] = torch.cos(pos * divide_val)\n",
    "        return positional_encodeing\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        pos_encoding = self.positional_encodeing[:seq_len, :]\n",
    "        print(x.size())\n",
    "        print(pos_encoding.size())\n",
    "        x = x + pos_encoding\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "025a30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, embedding_vectors, max_seq_length, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embedding_vectors).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.positional_encoding = PositionalEncoding(max_seq_length, d_model)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        sentence = self.embedding(torch.from_numpy(sentence))\n",
    "        sentence = self.positional_encoding(sentence)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b70d020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([628, 100])\n",
      "torch.Size([628, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2278,  0.4277, -0.2627,  ...,  0.7555, -0.0475,  1.4408],\n",
       "        [ 0.8798,  0.9391,  0.8815,  ...,  1.1677, -0.0870,  1.2921],\n",
       "        [ 0.3637,  0.6804,  2.4960,  ...,  0.0963,  0.4837,  1.0304],\n",
       "        ...,\n",
       "        [ 0.1760, -0.9844, -0.7557,  ...,  0.6794,  0.6576,  0.7534],\n",
       "        [-0.7332, -0.6800, -0.9877,  ...,  0.6784,  0.6584,  0.7526],\n",
       "        [-0.9683,  0.2496, -0.5167,  ...,  0.6774,  0.6593,  0.7519]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_test():\n",
    "    data_set = TextDataset(eng_vocab, train_data_x)\n",
    "    sentence = data_set[0]\n",
    "    max_seq_length = len(sentence)\n",
    "    d_model = eng_emb.shape[1]\n",
    "    embedding_layer = Embedding(eng_emb, max_seq_length, d_model)\n",
    "    return embedding_layer(sentence)\n",
    "embedding_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ac45955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f14229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
