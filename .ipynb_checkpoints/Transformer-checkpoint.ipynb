{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9628c6e7",
   "metadata": {},
   "source": [
    "In this article Transformer Network is implemented in Pytorch based on \"Attention is all you need\" paper. \n",
    "The first motivation of developing such network that is the sequential analysis is a bottleneck in sequential datas. What a transformer network is trying to do, is combining CNN and RNN specially attention mechanism, create a network that solves this bottelneck.\n",
    "The network consist of some parts.\n",
    "1. self attention:\n",
    "    an attention mechanism relating to diffrent parts of a sequence.\n",
    "2. Encoder\n",
    "    the encoder block inputs (x1,...,xn) and outputs (z1,...,zn) and the decoder generates (y1,...,yn) based on input Z. Also encoder block consists of N=6 identical layers.\n",
    "    the encoder consists of two parts. a multi-head attention and a feed forward network. and some residual connections between them. The residual is a normalization layer over output of each block plus output of the privious block.\n",
    "    note that input dimentions (embedding dimention) of encoder is d=512 and to make use of residual connections we have to set output of each block to have same dimention of input so dimention of output of each block is d=512.\n",
    "3. Decoder:\n",
    "    The decoder is like encoder consists except it has a new multi-head layer to relate input of decoder to encoder features.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f7dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/anaconda3/envs/2dunet/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a51f84",
   "metadata": {},
   "source": [
    "First of all we need a scaled dot product. Typically what this layer does is that it computes softmax(Q * K.T / sqrt(d_k)) * V. This computes the value of output in regard to each query and key (question and answers which are created during training). multiplying with value.\n",
    "The shapes are like this. Q, K:(batch_size, sequence_number, dk) and V:(batch_size, sequence_number, dv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b02445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        self.activation = nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        x = torch.bmm(Q, K.transpose(-1, -2))\n",
    "        dk = torch.tensor(K.size(-1))\n",
    "        x = x.div(torch.sqrt(dk))\n",
    "        x = self.activation(x)\n",
    "        x = torch.bmm(x, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f35ae1",
   "metadata": {},
   "source": [
    "in order to test This class I create this paramters. Work on paper to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d78018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3., 3., 3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3., 3., 3.]],\n",
      "\n",
      "        [[3., 3., 3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3., 3., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "def test_scaled_dot_product():\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    d_k = 4\n",
    "    d_v = 6\n",
    "    Q = torch.full((batch_size, sequence_number, d_k), 1, dtype= torch.float)\n",
    "    K = torch.full((batch_size, sequence_number, d_k), 2, dtype= torch.float)\n",
    "    V = torch.full((batch_size, sequence_number, d_v), 3, dtype= torch.float)\n",
    "    scaled_dot_product = ScaledDotProduct()\n",
    "    product = scaled_dot_product(Q, K, V)\n",
    "#     torch_versio = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    print(product)\n",
    "#     print(torch_versio)\n",
    "test_scaled_dot_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6ef35",
   "metadata": {},
   "source": [
    "ok, we implemented scaled dot product. Now we need to implement multi-head attention using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c5df403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dk = 10\n",
    "        self.dv = 12\n",
    "        self.num_head = num_head\n",
    "        self.WQ = nn.Parameter(torch.randn(self.num_head, d_model, self.dk))\n",
    "        self.WK = nn.Parameter(torch.randn(self.num_head, d_model, self.dk))\n",
    "        self.WV = nn.Parameter(torch.randn(self.num_head, d_model, self.dv))\n",
    "        self.WO = nn.Parameter(torch.randn(self.num_head * self.dv, d_model))\n",
    "        self.reset_parameters()\n",
    "        self.scaled_dot_product = ScaledDotProduct()\n",
    "        \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.WQ)\n",
    "        nn.init.xavier_uniform_(self.WK)\n",
    "        nn.init.xavier_uniform_(self.WV)\n",
    "        \n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            WQi, WKi, WVi = self.WQ[i, :, :], self.WK[i, :, :], self.WV[i, :, :]\n",
    "            q = torch.bmm(Q, WQi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            k = torch.bmm(K, WKi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            v = torch.bmm(V, WVi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            heads.append(self.scaled_dot_product(q,k,v))\n",
    "        out = torch.cat(heads, dim=-1)\n",
    "        out = torch.bmm(out, self.WO.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb3c9a4",
   "metadata": {},
   "source": [
    "In order to test the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3dd37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940],\n",
      "         [-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940],\n",
      "         [-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940],\n",
      "         [-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940],\n",
      "         [-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940]],\n",
      "\n",
      "        [[-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940],\n",
      "         [-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940],\n",
      "         [-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940],\n",
      "         [-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940],\n",
      "         [-18.4419,  -6.6514,   9.7013,  ..., -13.9123, -19.1616,  14.5940]]],\n",
      "       grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    num_head = 8\n",
    "    d_model = 128\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    multi_head_attention = MultiHeadAttention(num_head, d_model)\n",
    "    Q = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    K = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    V = torch.full((batch_size, sequence_number, d_model), 3, dtype= torch.float)\n",
    "    out = multi_head_attention(Q, K, V)\n",
    "    print(out)\n",
    "    # use torch version to see if we were correct\n",
    "#     torch_multi_head_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_head, batch_first=True)\n",
    "#     torch_out = torch_multi_head_attention(Q, K, V, need_weights=False)\n",
    "#     print(torch_out[0])\n",
    "#     print(torch.eq(out, torch_out[0]).all())\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce069b92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f62f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.layer2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_num, _ = x.size()\n",
    "        x = x.view(-1, self.d_model)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(batch_size, sequence_num, self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a040426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
