{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9628c6e7",
   "metadata": {},
   "source": [
    "In this article Transformer Network is implemented in Pytorch based on \"Attention is all you need\" paper. \n",
    "The first motivation of developing such network that is the sequential analysis is a bottleneck in sequential datas. What a transformer network is trying to do, is combining CNN and RNN specially attention mechanism, create a network that solves this bottelneck.\n",
    "The network consist of some parts.\n",
    "1. self attention:\n",
    "    an attention mechanism relating to diffrent parts of a sequence.\n",
    "2. Encoder\n",
    "    the encoder block inputs (x1,...,xn) and outputs (z1,...,zn) and the decoder generates (y1,...,yn) based on input Z. Also encoder block consists of N=6 identical layers.\n",
    "    the encoder consists of two parts. a multi-head attention and a feed forward network. and some residual connections between them. The residual is a normalization layer over output of each block plus output of the privious block.\n",
    "    note that input dimentions (embedding dimention) of encoder is d=512 and to make use of residual connections we have to set output of each block to have same dimention of input so dimention of output of each block is d=512.\n",
    "3. Decoder:\n",
    "    The decoder is like encoder consists except it has a new multi-head layer to relate input of decoder to encoder features.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1f7dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/anaconda3/envs/2dunet/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a51f84",
   "metadata": {},
   "source": [
    "First of all we need a scaled dot product. Typically what this layer does is that it computes softmax(Q * K.T / sqrt(d_k)) * V. This computes the value of output in regard to each query and key (question and answers which are created during training). multiplying with value.\n",
    "The shapes are like this. Q, K:(batch_size, sequence_number, dk) and V:(batch_size, sequence_number, dv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b02445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        self.activation = nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        x = torch.bmm(Q, K.transpose(-1, -2))\n",
    "        dk = torch.tensor(K.size(-1))\n",
    "        x = x.div(torch.sqrt(dk))\n",
    "        x = self.activation(x)\n",
    "        x = torch.bmm(x, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f35ae1",
   "metadata": {},
   "source": [
    "in order to test This class I create this paramters. Work on paper to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20d78018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "def test_scaled_dot_product():\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    d_k = 4\n",
    "    d_v = 6\n",
    "    Q = torch.full((batch_size, sequence_number, d_k), 1, dtype= torch.float)\n",
    "    K = torch.full((batch_size, sequence_number, d_k), 2, dtype= torch.float)\n",
    "    V = torch.full((batch_size, sequence_number, d_v), 3, dtype= torch.float)\n",
    "    scaled_dot_product = ScaledDotProduct()\n",
    "    product = scaled_dot_product(Q, K, V)\n",
    "#     torch_versio = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    print(product.size())\n",
    "#     print(torch_versio)\n",
    "test_scaled_dot_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6ef35",
   "metadata": {},
   "source": [
    "ok, we implemented scaled dot product. Now we need to implement multi-head attention using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5df403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dk = 10\n",
    "        self.dv = 12\n",
    "        self.num_head = num_head\n",
    "        self.WQ = nn.Parameter(torch.randn(self.num_head, d_model, self.dk))\n",
    "        self.WK = nn.Parameter(torch.randn(self.num_head, d_model, self.dk))\n",
    "        self.WV = nn.Parameter(torch.randn(self.num_head, d_model, self.dv))\n",
    "        self.WO = nn.Parameter(torch.randn(self.num_head * self.dv, d_model))\n",
    "        self.reset_parameters()\n",
    "        self.scaled_dot_product = ScaledDotProduct()\n",
    "        \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.WQ)\n",
    "        nn.init.xavier_uniform_(self.WK)\n",
    "        nn.init.xavier_uniform_(self.WV)\n",
    "        \n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            WQi, WKi, WVi = self.WQ[i, :, :], self.WK[i, :, :], self.WV[i, :, :]\n",
    "            q = torch.bmm(Q, WQi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            k = torch.bmm(K, WKi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            v = torch.bmm(V, WVi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            heads.append(self.scaled_dot_product(q,k,v))\n",
    "        out = torch.cat(heads, dim=-1)\n",
    "        out = torch.bmm(out, self.WO.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb3c9a4",
   "metadata": {},
   "source": [
    "In order to test the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3dd37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    num_head = 8\n",
    "    d_model = 128\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    multi_head_attention = MultiHeadAttention(num_head, d_model)\n",
    "    Q = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    K = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    V = torch.full((batch_size, sequence_number, d_model), 3, dtype= torch.float)\n",
    "    out = multi_head_attention(Q, K, V)\n",
    "    print(out.size())\n",
    "    # use torch version to see if we were correct\n",
    "#     torch_multi_head_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_head, batch_first=True)\n",
    "#     torch_out = torch_multi_head_attention(Q, K, V, need_weights=False)\n",
    "#     print(torch_out[0])\n",
    "#     print(torch.eq(out, torch_out[0]).all())\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0ca78",
   "metadata": {},
   "source": [
    "The next block is a simple feed forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f62f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.layer2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.size()\n",
    "        x = x.view(-1, self.d_model)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a040426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_feed_forward():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, num_head, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    feed_forward = FeedForward(d_model, d_ff)\n",
    "    out = feed_forward(x)\n",
    "    print(out.size())\n",
    "test_feed_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31e075",
   "metadata": {},
   "source": [
    "Now we have all bulding blocks to create encoder and decoder. The residual connections are applied too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc73ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_head, d_model, d_ff):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.num_head, self.d_model, self.d_ff = num_head, d_model, d_ff\n",
    "        self.multi_head_attention1 = MultiHeadAttention(num_head, d_model)\n",
    "        self.feed_forward1 = FeedForward(d_model, d_ff)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "    def batch_norm(self, x):\n",
    "        x_temp = x.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1d(x_temp)\n",
    "        x = x_temp.view(x.size())\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.multi_head_attention1(x, x, x)\n",
    "        x1 = torch.add(x, x1)\n",
    "        \n",
    "        x_temp = x1.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1(x_temp)\n",
    "        x1 = x_temp.view(x1.size())\n",
    "        \n",
    "        x2 = self.feed_forward1(x1)\n",
    "        x2 = torch.add(x2, x1)\n",
    "        \n",
    "        x_temp = x2.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm2(x_temp)\n",
    "        x2 = x_temp.view(x2.size())\n",
    "        \n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a5c46",
   "metadata": {},
   "source": [
    "and in order to test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f3ed046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_block():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder_block = EncoderBlock(num_head, d_model, d_ff)\n",
    "    out = encoder_block(x)\n",
    "    print(out.size())\n",
    "test_encoder_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d21c38",
   "metadata": {},
   "source": [
    "In the next block, we implement the stack of encoder blocks. N number of blocks are stacked on the top of each other to create Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26af5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.block_list = nn.ModuleList([EncoderBlock(num_head, d_model, d_ff) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.block_list:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1838ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder = Encoder(N, num_head, d_model, d_ff)\n",
    "    out = encoder(x)\n",
    "    print(out.size())\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168edae0",
   "metadata": {},
   "source": [
    "The exact same process is applied on decoder. Note that decoder inputs are the input sentence and the output of encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16769756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_head, d_model, d_ff):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_head, self.d_model, self.d_ff = num_head, d_model, d_ff\n",
    "        self.multi_head_attention1 = MultiHeadAttention(num_head, d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(num_head, d_model)\n",
    "        self.feed_forward1 = FeedForward(d_model, d_ff)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, encoder_out):\n",
    "        x1 = self.multi_head_attention1(x, x, x)\n",
    "        x1 = torch.add(x, x1)\n",
    "        \n",
    "        x_temp = x1.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1(x_temp)\n",
    "        x1 = x_temp.view(x1.size())\n",
    "        \n",
    "        x2 = self.multi_head_attention2(x1, encoder_out, encoder_out)\n",
    "        x2 = torch.add(x1, x2)\n",
    "        \n",
    "        x_temp = x2.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm2(x_temp)\n",
    "        x2 = x_temp.view(x2.size())\n",
    "        \n",
    "        x3 = self.feed_forward1(x2)\n",
    "        x3 = torch.add(x2, x3)\n",
    "        \n",
    "        x_temp = x3.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm3(x_temp)\n",
    "        x3 = x_temp.view(x3.size())\n",
    "        \n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84b38c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_block():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encode_out = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    decoder_block = DecoderBlock(num_head, d_model, d_ff)\n",
    "    out = decoder_block(x, encode_out)\n",
    "    print(out.size())\n",
    "test_encoder_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ebffa16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.block_list = nn.ModuleList([DecoderBlock(num_head, d_model, d_ff) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x, encoder_out):\n",
    "        for block in self.block_list:\n",
    "            x = block(x, encoder_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f085bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_decoder():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder_out = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    decoder = Decoder(N, num_head, d_model, d_ff)\n",
    "    out = decoder(x, encoder_out)\n",
    "    print(out.size())\n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb2c99",
   "metadata": {},
   "source": [
    "Now its time to put everything together and create the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da5c4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(N, num_head, d_model, d_ff)\n",
    "        self.decoder = Decoder(N, num_head, d_model, d_ff)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_out = self.encoder(x)\n",
    "        out = self.decoder(x, encoder_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6034ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_transformer():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    \n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    transformer = Transformer(N, num_head, d_model, d_ff)\n",
    "    out = transformer(x)\n",
    "    \n",
    "    print(out.size())\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2f389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
