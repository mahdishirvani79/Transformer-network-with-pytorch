{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9628c6e7",
   "metadata": {},
   "source": [
    "In this article Transformer Network is implemented in Pytorch based on \"Attention is all you need\" paper. \n",
    "The first motivation of developing such network that is the sequential analysis is a bottleneck in sequential datas. What a transformer network is trying to do, is combining CNN and RNN specially attention mechanism, create a network that solves this bottelneck.\n",
    "The network consist of some parts.\n",
    "1. self attention:\n",
    "    an attention mechanism relating to diffrent parts of a sequence.\n",
    "2. Encoder\n",
    "    the encoder block inputs (x1,...,xn) and outputs (z1,...,zn) and the decoder generates (y1,...,yn) based on input Z. Also encoder block consists of N=6 identical layers.\n",
    "    the encoder consists of two parts. a multi-head attention and a feed forward network. and some residual connections between them. The residual is a normalization layer over output of each block plus output of the privious block.\n",
    "    note that input dimentions (embedding dimention) of encoder is d=512 and to make use of residual connections we have to set output of each block to have same dimention of input so dimention of output of each block is d=512.\n",
    "3. Decoder:\n",
    "    The decoder is like encoder consists except it has a new multi-head layer to relate input of decoder to encoder features.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f7dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/anaconda3/envs/2dunet/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a51f84",
   "metadata": {},
   "source": [
    "First of all we need a scaled dot product. Typically what this layer does is that it computes softmax(Q * K.T / sqrt(d_k)) * V. This computes the value of output in regard to each query and key (question and answers which are created during training). multiplying with value.\n",
    "The shapes are like this. Q, K:(batch_size, sequence_number, dk) and V:(batch_size, sequence_number, dv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b02445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        self.activation = nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        x = torch.bmm(Q, K.transpose(-1, -2))\n",
    "        dk = torch.tensor(K.size(-1))\n",
    "        x = x.div(torch.sqrt(dk))\n",
    "        x = self.activation(x)\n",
    "        x = torch.bmm(x, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f35ae1",
   "metadata": {},
   "source": [
    "in order to test This class I create this paramters. Work on paper to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d78018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "def test_scaled_dot_product():\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    d_k = 4\n",
    "    d_v = 6\n",
    "    Q = torch.full((batch_size, sequence_number, d_k), 1, dtype= torch.float)\n",
    "    K = torch.full((batch_size, sequence_number, d_k), 2, dtype= torch.float)\n",
    "    V = torch.full((batch_size, sequence_number, d_v), 3, dtype= torch.float)\n",
    "    scaled_dot_product = ScaledDotProduct()\n",
    "    product = scaled_dot_product(Q, K, V)\n",
    "#     torch_versio = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    print(product.size())\n",
    "#     print(torch_versio)\n",
    "test_scaled_dot_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6ef35",
   "metadata": {},
   "source": [
    "ok, we implemented scaled dot product. Now we need to implement multi-head attention using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5df403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dk = 10\n",
    "        self.dv = 12\n",
    "        self.num_head = num_head\n",
    "        self.WQ = nn.Parameter(torch.randn(self.num_head, d_model, self.dk))\n",
    "        self.WK = nn.Parameter(torch.randn(self.num_head, d_model, self.dk))\n",
    "        self.WV = nn.Parameter(torch.randn(self.num_head, d_model, self.dv))\n",
    "        self.WO = nn.Parameter(torch.randn(self.num_head * self.dv, d_model))\n",
    "        self.reset_parameters()\n",
    "        self.scaled_dot_product = ScaledDotProduct()\n",
    "        \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.WQ)\n",
    "        nn.init.xavier_uniform_(self.WK)\n",
    "        nn.init.xavier_uniform_(self.WV)\n",
    "        \n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            WQi, WKi, WVi = self.WQ[i, :, :], self.WK[i, :, :], self.WV[i, :, :]\n",
    "            q = torch.bmm(Q, WQi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            k = torch.bmm(K, WKi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            v = torch.bmm(V, WVi.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "            heads.append(self.scaled_dot_product(q,k,v))\n",
    "        out = torch.cat(heads, dim=-1)\n",
    "        out = torch.bmm(out, self.WO.unsqueeze(0).repeat(Q.size(0), 1, 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb3c9a4",
   "metadata": {},
   "source": [
    "In order to test the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3dd37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    num_head = 8\n",
    "    d_model = 128\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    multi_head_attention = MultiHeadAttention(num_head, d_model)\n",
    "    Q = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    K = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    V = torch.full((batch_size, sequence_number, d_model), 3, dtype= torch.float)\n",
    "    out = multi_head_attention(Q, K, V)\n",
    "    print(out.size())\n",
    "    # use torch version to see if we were correct\n",
    "#     torch_multi_head_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_head, batch_first=True)\n",
    "#     torch_out = torch_multi_head_attention(Q, K, V, need_weights=False)\n",
    "#     print(torch_out[0])\n",
    "#     print(torch.eq(out, torch_out[0]).all())\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0ca78",
   "metadata": {},
   "source": [
    "The next block is a simple feed forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f62f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.layer2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.size()\n",
    "        x = x.view(-1, self.d_model)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a040426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_feed_forward():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, num_head, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    feed_forward = FeedForward(d_model, d_ff)\n",
    "    out = feed_forward(x)\n",
    "    print(out.size())\n",
    "test_feed_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7df76",
   "metadata": {},
   "source": [
    "Now we have all bulding blocks to create encoder and decoder. The residual connections are applied too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958e8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_head, d_model, d_ff):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.num_head, self.d_model, self.d_ff = num_head, d_model, d_ff\n",
    "        self.multi_head_attention1 = MultiHeadAttention(num_head, d_model)\n",
    "        self.feed_forward1 = FeedForward(d_model, d_ff)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "    def batch_norm(self, x):\n",
    "        x_temp = x.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1d(x_temp)\n",
    "        x = x_temp.view(x.size())\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.multi_head_attention1(x, x, x)\n",
    "        x1 = torch.add(x, x1)\n",
    "        \n",
    "        x_temp = x1.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1(x_temp)\n",
    "        x1 = x_temp.view(x1.size())\n",
    "        \n",
    "        x2 = self.feed_forward1(x1)\n",
    "        x2 = torch.add(x2, x1)\n",
    "        \n",
    "        x_temp = x2.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm2(x_temp)\n",
    "        x2 = x_temp.view(x2.size())\n",
    "        \n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcba99",
   "metadata": {},
   "source": [
    "and in order to test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3314068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_block():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder_block = EncoderBlock(num_head, d_model, d_ff)\n",
    "    out = encoder_block(x)\n",
    "    print(out.size())\n",
    "test_encoder_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae251dc7",
   "metadata": {},
   "source": [
    "In the next block, we implement the stack of encoder blocks. N number of blocks are stacked on the top of each other to create Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c36b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.block_list = nn.ModuleList([EncoderBlock(num_head, d_model, d_ff) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.block_list:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a006a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder = Encoder(N, num_head, d_model, d_ff)\n",
    "    out = encoder(x)\n",
    "    print(out.size())\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eec78",
   "metadata": {},
   "source": [
    "The exact same process is applied on decoder. Note that decoder inputs are the input sentence and the output of encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05801ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_head, d_model, d_ff):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_head, self.d_model, self.d_ff = num_head, d_model, d_ff\n",
    "        self.multi_head_attention1 = MultiHeadAttention(num_head, d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(num_head, d_model)\n",
    "        self.feed_forward1 = FeedForward(d_model, d_ff)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(d_model)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, encoder_out):\n",
    "        x1 = self.multi_head_attention1(x, x, x)\n",
    "        x1 = torch.add(x, x1)\n",
    "        \n",
    "        x_temp = x1.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm1(x_temp)\n",
    "        x1 = x_temp.view(x1.size())\n",
    "        \n",
    "        x2 = self.multi_head_attention2(x1, encoder_out, encoder_out)\n",
    "        x2 = torch.add(x1, x2)\n",
    "        \n",
    "        x_temp = x2.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm2(x_temp)\n",
    "        x2 = x_temp.view(x2.size())\n",
    "        \n",
    "        x3 = self.feed_forward1(x2)\n",
    "        x3 = torch.add(x2, x3)\n",
    "        \n",
    "        x_temp = x3.view(-1, self.d_model)\n",
    "        x_temp = self.batch_norm3(x_temp)\n",
    "        x3 = x_temp.view(x3.size())\n",
    "        \n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00e57b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_block():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encode_out = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    decoder_block = DecoderBlock(num_head, d_model, d_ff)\n",
    "    out = decoder_block(x, encode_out)\n",
    "    print(out.size())\n",
    "test_encoder_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b84c6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.block_list = nn.ModuleList([DecoderBlock(num_head, d_model, d_ff) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x, encoder_out):\n",
    "        for block in self.block_list:\n",
    "            x = block(x, encoder_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3560e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_decoder():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    x = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    encoder_out = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    decoder = Decoder(N, num_head, d_model, d_ff)\n",
    "    out = decoder(x, encoder_out)\n",
    "    print(out.size())\n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a588ea",
   "metadata": {},
   "source": [
    "Now its time to put everything together and create the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98d9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N, num_head, d_model, d_ff):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(N, num_head, d_model, d_ff)\n",
    "        self.decoder = Decoder(N, num_head, d_model, d_ff)\n",
    "        \n",
    "    def forward(self, x_encoder, x_decoder):\n",
    "        encoder_out = self.encoder(x_encoder)\n",
    "        out = self.decoder(x_decoder, encoder_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66c9dfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_transformer():\n",
    "    num_head = 8\n",
    "    d_model = 512\n",
    "    d_ff = 2045\n",
    "    batch_size = 2\n",
    "    sequence_number = 5\n",
    "    N = 6\n",
    "    \n",
    "    x_encoder = torch.full((batch_size, sequence_number, d_model), 1, dtype= torch.float)\n",
    "    x_decoder = torch.full((batch_size, sequence_number, d_model), 2, dtype= torch.float)\n",
    "    transformer = Transformer(N, num_head, d_model, d_ff)\n",
    "    out = transformer(x_encoder, x_decoder)\n",
    "    \n",
    "    print(out.size())\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ced7e",
   "metadata": {},
   "source": [
    "Now the model is created. We have to see how to feed data. The first step is to create word embedding. function below created word embedding. Note that \\<pad> and and \\<unk> tokens are not in glove and we add them as pad as zeros and unk as mean of all vectors. The \\<unk> part is now commented because sounds like glove solved it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b31efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(path: str, pad=False, unk=False):\n",
    "    vocab, embeddings = dict(), list()\n",
    "    with open(f'{path}', 'rt') as f:\n",
    "        full_content = f.read().strip().split('\\n')\n",
    "    for i in range(len(full_content)):\n",
    "        i_word = full_content[i].split(' ')[0]\n",
    "        i_embedding = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "        vocab[i_word] = i\n",
    "        embeddings.append(i_embedding)\n",
    "    embs_npa = np.array(embeddings)\n",
    "    if pad:\n",
    "        vocab['<pad>'] = len(vocab)\n",
    "        pad_emb_npa = np.zeros((1, embs_npa.shape[1]))\n",
    "        embs_npa = np.vstack((embs_npa, pad_emb_npa))\n",
    "    if unk:\n",
    "        vocab['<unk>'] = len(vocab)\n",
    "        unk_emb_npa = np.mean(embs_npa, axis=0, keepdims=True)\n",
    "        embs_npa = np.vstack((embs_npa, unk_emb_npa))\n",
    "    return vocab, embs_npa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e2ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab, eng_emb = get_embedding(\"./word2vec_eng/glove.6B.100d.txt\", pad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f19e4",
   "metadata": {},
   "source": [
    "In this model we create a layer that accepts a sentence and give its tokenized outputs. for the sake of learning this layer is not efficient. But it should be enhanced by torchtext for tokenizing and dataloader for input a sentence and output the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0914f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, word2indx, embedding_vectors):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.word2indx = word2indx\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embedding_vectors).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, sentence:str):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.split()\n",
    "        indexes = [self.word2indx[x] for x in sentence]\n",
    "        return self.embedding(torch.tensor(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "354df4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.0849e-04,  3.8636e-01,  9.1302e-01,  2.6519e-01, -6.4381e-01,\n",
      "          5.3025e-01,  5.7382e-02, -4.6133e-01,  5.4982e-01,  2.8236e-01,\n",
      "         -2.0539e-01,  4.0812e-01, -1.5084e-01, -7.0599e-01,  4.8303e-01,\n",
      "          3.8221e-01, -6.4972e-01,  3.4158e-01,  3.4914e-01,  7.3472e-01,\n",
      "          1.7834e-01,  2.4603e-01,  4.0430e-01, -4.4673e-01, -3.7324e-01,\n",
      "          5.0797e-01, -1.6289e-01, -3.4801e-01, -1.4731e-01,  1.9367e-01,\n",
      "         -1.6281e-02,  8.3232e-01, -1.3749e-01, -4.2175e-01,  2.4230e-01,\n",
      "          3.3853e-01, -9.8536e-01,  1.0919e-01,  9.4898e-01, -4.1409e-01,\n",
      "         -2.8837e-01,  1.9599e-01, -1.5610e-01,  1.1042e-01, -4.6167e-01,\n",
      "         -9.8950e-01,  6.3015e-01,  6.6713e-01, -3.9951e-01, -1.7624e-01,\n",
      "          7.5891e-02, -4.0964e-01,  4.0504e-01,  2.4167e-01, -5.9434e-01,\n",
      "         -5.9167e-01,  4.7133e-01,  8.2302e-02, -2.3496e-01, -7.1120e-01,\n",
      "          5.8578e-02,  2.1652e-01, -5.9794e-01,  3.5182e-02,  4.4287e-01,\n",
      "         -3.9261e-01,  6.9230e-01,  5.3259e-01, -2.7644e-01, -6.9603e-01,\n",
      "          2.3902e-01, -3.1415e-01, -3.6365e-01, -2.4944e-01, -2.1407e-01,\n",
      "          4.7531e-02,  4.7696e-01,  6.9801e-01, -1.7188e-01, -8.4868e-01,\n",
      "          9.0577e-02,  5.6525e-01, -2.2183e-01,  1.8580e-01, -9.9258e-01,\n",
      "         -1.4941e-01, -1.2927e-01,  6.1605e-01, -6.5291e-01, -8.9528e-01,\n",
      "         -6.2088e-01,  2.0754e-01,  1.4321e-01,  1.8025e-01, -1.4311e-01,\n",
      "         -2.8145e-01, -9.7145e-01, -7.6650e-01, -1.0677e+00,  3.9524e-01],\n",
      "        [-5.8342e-01,  4.8631e-01,  7.4230e-01,  1.7875e-01, -1.5873e+00,\n",
      "         -3.7499e-01,  3.7902e-01,  7.0767e-01, -1.5402e+00,  7.4851e-01,\n",
      "         -6.9018e-03, -4.4981e-01, -1.1125e-01,  1.7395e-01,  2.6005e-01,\n",
      "          2.7065e-01,  7.8731e-01,  8.4877e-01, -9.9277e-02,  1.7688e-02,\n",
      "          4.4389e-01, -8.7300e-01,  4.9640e-01, -1.8430e-01,  2.4894e-01,\n",
      "          2.4073e-01,  8.4460e-02,  7.0786e-02, -2.3216e-01,  2.6685e-01,\n",
      "         -2.3519e-01,  4.5211e-01, -3.9982e-01,  2.4936e-01,  7.3548e-01,\n",
      "         -7.2352e-02, -8.1008e-01, -1.5256e-01, -1.0313e+00,  2.3067e-01,\n",
      "         -1.1634e+00,  2.0387e-01,  4.2369e-01, -1.0589e+00,  2.9905e-01,\n",
      "          2.0036e-01,  6.3371e-01, -5.7507e-01, -4.3730e-01, -5.5908e-01,\n",
      "          5.0811e-01,  3.2673e-01, -4.3710e-01,  1.0702e+00, -1.0418e-01,\n",
      "         -2.4432e+00, -1.0739e+00,  1.0881e-01,  1.5446e+00, -1.5633e-01,\n",
      "         -3.7779e-01,  1.5111e-01,  5.8397e-01,  5.4980e-01,  2.3775e-01,\n",
      "          8.8690e-01, -1.0220e-01, -4.1313e-02, -1.9496e-01, -1.3760e-01,\n",
      "         -1.3191e-04,  6.9490e-01,  6.6230e-02, -2.5647e-01, -1.0539e-01,\n",
      "          5.6950e-01,  1.7414e-01, -5.9899e-01, -8.6757e-01, -3.1254e-01,\n",
      "          5.4560e-01, -9.2359e-02, -1.2294e-01, -1.9419e-01, -1.9168e+00,\n",
      "         -4.4395e-02,  6.8561e-01,  2.0030e-01, -3.7791e-01,  6.7436e-01,\n",
      "         -7.7218e-01,  4.9596e-01,  5.8190e-03,  5.3833e-01,  3.3367e-01,\n",
      "          9.7822e-01,  3.1984e-01, -1.2619e-01, -1.7724e-02,  1.8389e-01]])\n"
     ]
    }
   ],
   "source": [
    "def naive_embedding_test():\n",
    "    sentence = \"naive test\"\n",
    "    embedding = Embedding(eng_vocab, eng_emb)\n",
    "    out = embedding(sentence)\n",
    "    print(out)\n",
    "naive_embedding_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30279503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea884fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.046539 ,  0.61966  ,  0.56647  , -0.46584  , -1.189    ,\n",
       "        0.44599  ,  0.066035 ,  0.3191   ,  0.14679  , -0.22119  ,\n",
       "        0.79239  ,  0.29905  ,  0.16073  ,  0.025324 ,  0.18678  ,\n",
       "       -0.31001  , -0.28108  ,  0.60515  , -1.0654   ,  0.52476  ,\n",
       "        0.064152 ,  1.0358   , -0.40779  , -0.38011  ,  0.30801  ,\n",
       "        0.59964  , -0.26991  , -0.76035  ,  0.94222  , -0.46919  ,\n",
       "       -0.18278  ,  0.90652  ,  0.79671  ,  0.24825  ,  0.25713  ,\n",
       "        0.6232   , -0.44768  ,  0.65357  ,  0.76902  , -0.51229  ,\n",
       "       -0.44333  , -0.21867  ,  0.3837   , -1.1483   , -0.94398  ,\n",
       "       -0.15062  ,  0.30012  , -0.57806  ,  0.20175  , -1.6591   ,\n",
       "       -0.079195 ,  0.026423 ,  0.22051  ,  0.99714  , -0.57539  ,\n",
       "       -2.7266   ,  0.31448  ,  0.70522  ,  1.4381   ,  0.99126  ,\n",
       "        0.13976  ,  1.3474   , -1.1753   ,  0.0039503,  1.0298   ,\n",
       "        0.064637 ,  0.90887  ,  0.82872  , -0.47003  , -0.10575  ,\n",
       "        0.5916   , -0.4221   ,  0.57331  , -0.54114  ,  0.10768  ,\n",
       "        0.39784  , -0.048744 ,  0.064596 , -0.61437  , -0.286    ,\n",
       "        0.5067   , -0.49758  , -0.8157   ,  0.16408  , -1.963    ,\n",
       "       -0.26693  , -0.37593  , -0.95847  , -0.8584   , -0.71577  ,\n",
       "       -0.32343  , -0.43121  ,  0.41392  ,  0.28374  , -0.70931  ,\n",
       "        0.15003  , -0.2154   , -0.37616  , -0.032502 ,  0.8062   ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_emb[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875994a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
